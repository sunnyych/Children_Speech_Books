{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('/Users/sunnyyu/Desktop/research/children_speech_books/code/python/generics/MERGED_DATA_SPEECH.json', 'r') as file:\n",
    "    generics_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360341"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generics_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 70\u001b[0m\n\u001b[1;32m     66\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(data, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     69\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myour_output_file.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 70\u001b[0m \u001b[43mprocess_json_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerics_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 51\u001b[0m, in \u001b[0;36mprocess_json_file\u001b[0;34m(data, output_file)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m book \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sentence_data \u001b[38;5;129;01min\u001b[39;00m book:\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;66;03m# Get the sentence and subject from the sentence data\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m         sentence \u001b[38;5;241m=\u001b[39m \u001b[43msentence_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m         subjects \u001b[38;5;241m=\u001b[39m sentence_data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubjects\u001b[39m\u001b[38;5;124m'\u001b[39m, [])\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m subjects:\n\u001b[1;32m     54\u001b[0m             \u001b[38;5;66;03m# Assume the first subject is the main one (you can adjust if needed)\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import tqdm as tqdm\n",
    "\n",
    "def categorize_sentence_by_subject(sentence, subject):\n",
    "    # Augmented lists of universal and existential quantifiers\n",
    "    universal_quantifiers = [\n",
    "        \"all\", \"every\", \"each\", \"any\", \"no\", \"none\", \"never\", \"always\", \n",
    "        \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"no one\", \n",
    "        \"nobody\", \"nothing\", \"neither\", \"every single\", \"without exception\", \n",
    "        \"entire\", \"whole\", \"everywhere\", \"whenever\"\n",
    "    ]\n",
    "    \n",
    "    existential_quantifiers = [\n",
    "        \"some\", \"a\", \"an\", \"one\", \"several\", \"many\", \"few\", \"a few\", \n",
    "        \"a couple of\", \"at least one\", \"a number of\", \"a lot of\", \n",
    "        \"a majority of\", \"a minority of\", \"certain\", \"most\", \"almost all\", \n",
    "        \"more than half\", \"half of\", \"less than half\", \"two-thirds\", \n",
    "        \"three-quarters\", \"dozens of\", \"hundreds of\", \"thousands of\", \n",
    "        \"millions of\", \"billions of\", \"about\", \"approximately\", \"various\", \n",
    "        \"plenty of\", \"somebody\", \"someone\", \"something\", \"somewhere\", \n",
    "        \"multiple\", \"varied\", \"part of\", \"enough\", \"much\", \n",
    "        \"a portion of\", \"particular\", \"certain individuals\", \"occasional\"\n",
    "    ]\n",
    "    \n",
    "    # Normalize the sentence and subject to lowercase for easier matching\n",
    "    sentence = sentence.lower()\n",
    "    subject = subject.lower()\n",
    "    \n",
    "    # Create a pattern to match quantifiers directly before the subject\n",
    "    universal_pattern = r'\\b(?:' + '|'.join(map(re.escape, universal_quantifiers)) + r')\\b\\s+\\b' + re.escape(subject) + r'\\b'\n",
    "    existential_pattern = r'\\b(?:' + '|'.join(map(re.escape, existential_quantifiers)) + r')\\b\\s+\\b' + re.escape(subject) + r'\\b'\n",
    "    \n",
    "    # Check if any universal quantifier appears directly before the subject\n",
    "    if re.search(universal_pattern, sentence):\n",
    "        return \"Universal Quantifier\"\n",
    "    \n",
    "    # Check if any existential quantifier appears directly before the subject\n",
    "    if re.search(existential_pattern, sentence):\n",
    "        return \"Existential Quantifier\"\n",
    "    \n",
    "    # If no quantifiers are found directly before the subject\n",
    "    return \"Neither\"\n",
    "\n",
    "def process_json_file(data, output_file):\n",
    "    \n",
    "    # Process each book\n",
    "    for book in data:\n",
    "        for sentence_data in book:\n",
    "            # Get the sentence and subject from the sentence data\n",
    "            sentence = sentence_data.get('sentence', \"\")\n",
    "            subjects = sentence_data.get('subjects', [])\n",
    "            if subjects:\n",
    "                # Assume the first subject is the main one (you can adjust if needed)\n",
    "                subject_text = subjects[0].get('subject', \"\")\n",
    "                # Categorize the sentence based on the subject\n",
    "                classification = categorize_sentence_by_subject(sentence, subject_text)\n",
    "                # Add the classification to the sentence data\n",
    "                sentence_data['quantified_classification'] = classification\n",
    "            else:\n",
    "                # If no subject is present, classify as \"Neither\"\n",
    "                sentence_data['quantified_classification'] = \"Neither\"\n",
    "    \n",
    "    # Save the updated data to a new JSON file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "\n",
    "output_file = 'your_output_file.json'\n",
    "process_json_file(generics_data, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing books: 100%|██████████| 360341/360341 [00:11<00:00, 30029.07it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Combine the original excluded pronouns with NLTK's stop words\n",
    "excluded_pronouns = {\n",
    "    'he', 'you', 'we', 'she', 'i', 'they', 'him', 'her', 'them', 'his', 'hers', 'theirs', 'it', 'way', 'thing'\n",
    "}\n",
    "excluded_pronouns.update(set(stopwords.words('english')))\n",
    "\n",
    "\n",
    "def categorize_sentence_by_subject(sentence, subject, universal_pattern, existential_pattern):\n",
    "    # Normalize the sentence and subject to lowercase for easier matching\n",
    "    sentence = sentence.lower()\n",
    "    subject = subject.lower()\n",
    "\n",
    "    # Check if any universal quantifier appears directly before the subject\n",
    "    if re.search(universal_pattern, sentence):\n",
    "        return \"Universal Quantifier\"\n",
    "    \n",
    "    # Check if any existential quantifier appears directly before the subject\n",
    "    if re.search(existential_pattern, sentence):\n",
    "        return \"Existential Quantifier\"\n",
    "    \n",
    "    # If no quantifiers are found directly before the subject\n",
    "    return \"Neither\"\n",
    "\n",
    "def process_json_file(data, output_file):\n",
    "    # Augmented lists of universal and existential quantifiers\n",
    "    universal_quantifiers = [\n",
    "        \"all\", \"every\", \"each\", \"any\", \"no\", \"none\", \"never\", \"always\", \n",
    "        \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"no one\", \n",
    "        \"nobody\", \"nothing\", \"neither\", \"every single\", \"without exception\", \n",
    "        \"entire\", \"whole\", \"everywhere\", \"whenever\"\n",
    "    ]\n",
    "    \n",
    "    existential_quantifiers = [\n",
    "        \"some\", \"a\", \"an\", \"one\", \"several\", \"many\", \"few\", \"a few\", \n",
    "        \"a couple of\", \"at least one\", \"a number of\", \"a lot of\", \n",
    "        \"a majority of\", \"a minority of\", \"certain\", \"most\", \"almost all\", \n",
    "        \"more than half\", \"half of\", \"less than half\", \"two-thirds\", \n",
    "        \"three-quarters\", \"dozens of\", \"hundreds of\", \"thousands of\", \n",
    "        \"millions of\", \"billions of\", \"about\", \"approximately\", \"various\", \n",
    "        \"plenty of\", \"somebody\", \"someone\", \"something\", \"somewhere\", \n",
    "        \"multiple\", \"varied\", \"part of\", \"enough\", \"much\", \n",
    "        \"a portion of\", \"particular\", \"certain individuals\", \"occasional\"\n",
    "    ]\n",
    "\n",
    "    # Process each book\n",
    "    for sentence_data in tqdm(data, desc=\"Processing books\"):\n",
    "            # Get the sentence and subject from the sentence data\n",
    "        sentence = sentence_data.get('sentence', \"\")\n",
    "        subjects = sentence_data.get('subjects', [])\n",
    "        if subjects:\n",
    "                # Assume the first subject is the main one (you can adjust if needed)\n",
    "            subject_text = subjects[0].get('subject', \"\")\n",
    "            if subject_text in excluded_pronouns:\n",
    "                sentence_data['quantified_classification'] = \"Neither\"\n",
    "            else:    \n",
    "                # Compile the regular expressions using the current subject\n",
    "                universal_pattern = re.compile(r'\\b(?:' + '|'.join(map(re.escape, universal_quantifiers)) + r')\\b\\s+\\b' + re.escape(subject_text) + r'\\b')\n",
    "                existential_pattern = re.compile(r'\\b(?:' + '|'.join(map(re.escape, existential_quantifiers)) + r')\\b\\s+\\b' + re.escape(subject_text) + r'\\b')\n",
    "\n",
    "                    # Categorize the sentence based on the subject\n",
    "                classification = categorize_sentence_by_subject(sentence, subject_text, universal_pattern, existential_pattern)\n",
    "                    # Add the classification to the sentence data\n",
    "                sentence_data['quantified_classification'] = classification\n",
    "        else:\n",
    "                # If no subject is present, classify as \"Neither\"\n",
    "            sentence_data['quantified_classification'] = \"Neither\"\n",
    "    \n",
    "    # Save the updated data to a new JSON file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "output_file = 'SPEECH_QUANTIFIERS.json'\n",
    "process_json_file(generics_data, output_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
