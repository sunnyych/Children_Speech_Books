{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"data/dependency_parsing_data/results/annotate_clauses_parents_final.json\", \"r\") as f:\n",
    "    clauses_parents = json.load(f)\n",
    "\n",
    "# for analyzing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import stanza\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize the Stanza pipeline\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "# Functions to find ancestors and descendants\n",
    "def find_ancestors(word, sentence_words):\n",
    "    ancestors = []\n",
    "    current = word\n",
    "    while current.head > 0:  # Head of 0 means the word is the root\n",
    "        current = sentence_words[current.head - 1]  # Head is 1-based index\n",
    "        ancestors.append(current.text)\n",
    "    return ancestors\n",
    "\n",
    "def find_descendants(word, sentence_words):\n",
    "    descendants = []\n",
    "    stack = [word]\n",
    "    while stack:\n",
    "        current = stack.pop()\n",
    "        for w in sentence_words:\n",
    "            if w.head == current.id:  # If the word points to the current word\n",
    "                descendants.append(w.text)\n",
    "                stack.append(w)\n",
    "    return descendants\n",
    "\n",
    "# Function to extract subjects and their related words from a clause\n",
    "def extract_subjects_and_related_words(clause_text):\n",
    "    doc = nlp(clause_text)\n",
    "    results = []\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            if word.deprel in ('nsubj', 'nsubjpass'):\n",
    "                result = {\n",
    "                    'subject': word.text,\n",
    "                    'words_pointing_to_subject': find_ancestors(word, sentence.words),\n",
    "                    'words_subject_points_to': find_descendants(word, sentence.words)\n",
    "                }\n",
    "                results.append(result)\n",
    "    return results\n",
    "\n",
    "# Function to process a single clause\n",
    "def process_clause(clause):\n",
    "    clause_text = clause['sentence']\n",
    "    clause['subjects'] = extract_subjects_and_related_words(clause_text)\n",
    "    return clause\n",
    "\n",
    "# Function to process clauses\n",
    "def process_clauses(clauses):\n",
    "    results = []\n",
    "    for clause in tqdm(clauses, desc=\"Processing clauses\"):\n",
    "        results.append(process_clause(clause))\n",
    "    return results\n",
    "\n",
    "\n",
    "# Process the clauses\n",
    "processed_clauses = process_clauses(annotated_clauses)\n",
    "\n",
    "# Save the processed clauses to a JSON file\n",
    "with open('dependency_parsing_parents.json', 'w') as f:\n",
    "    json.dump(processed_clauses, f, indent=4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map clause tags back to the annotated json with object category labels\n",
    "import json\n",
    "\n",
    "# Load the data from both JSON files\n",
    "with open('/Users/sunnyyu/Desktop/research/children_speech_books/code/python/generics/dependency_parsing.json', 'r') as file:\n",
    "    generics_data = json.load(file)\n",
    "\n",
    "with open('/Users/sunnyyu/Desktop/research/children_speech_books/code/python/analysis/annotated/annotated_books_clean.json', 'r') as file:\n",
    "    books_data = json.load(file)\n",
    "\n",
    "# Assuming both generics_data and books_data are lists of dictionaries\n",
    "# Create a dictionary for quick lookup by sentence_num from generics_data\n",
    "generics_dict = {item['sentence']: item for item in generics_data}\n",
    "\n",
    "# Merge the data\n",
    "for book in books_data:\n",
    "    for sentence in book:\n",
    "        sentence_str = sentence['sentence']\n",
    "        if sentence_str in generics_dict:\n",
    "            sentence.update({\n",
    "                'category': generics_dict[sentence_str]['category'],\n",
    "                'subjects': generics_dict[sentence_str]['subjects']\n",
    "            })\n",
    "\n",
    "# Save the merged data back to a new JSON file\n",
    "with open('merged_data.json', 'w') as file:\n",
    "    json.dump(books_data, file, indent=4)\n",
    "\n",
    "print(\"Data merged and saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge parents speech BIO tags with dependency parsing and situation entity type data\n",
    "\n",
    "# map clause tags back to the annotated json with object category labels\n",
    "import json\n",
    "\n",
    "# Load the data from both JSON files\n",
    "with open('/Users/sunnyyu/Desktop/research/children_speech_books/code/python/generics/dependency_parsing_parents_final_updated.json', 'r') as file:\n",
    "    generics_data = json.load(file)\n",
    "\n",
    "with open('/Users/sunnyyu/Desktop/research/children_speech_books/code/python/generics/childes_parent_clean.json', 'r') as file:\n",
    "    parents_data = json.load(file)\n",
    "\n",
    "# Assuming both generics_data and books_data are lists of dictionaries\n",
    "# Create a dictionary for quick lookup by sentence_num from generics_data\n",
    "generics_dict = {item['sentence']: item for item in parents_data}\n",
    "\n",
    "# Merge the data\n",
    "for sentence in generics_data:\n",
    "    sentence_str = sentence['sentence']\n",
    "    if sentence_str in generics_dict:\n",
    "        sentence.update({\n",
    "                'tags': generics_dict[sentence_str]['tags'],\n",
    "                'age_min': generics_dict[sentence_str]['age_min'],\n",
    "                'age_max': generics_dict[sentence_str]['age_max']\n",
    "            })\n",
    "\n",
    "# Save the merged data back to a new JSON file\n",
    "with open('merged_data_parents_speech.json', 'w') as file:\n",
    "    json.dump(generics_data, file, indent=4)\n",
    "\n",
    "print(\"Data merged and saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge parents speech data\n",
    "\n",
    "# add category to dependency parsing\n",
    "\n",
    "import json\n",
    "\n",
    "# Load JSON data\n",
    "with open('/Users/sunnyyu/Desktop/research/children_speech_books/code/python/generics/parents_generics/results/annotate_clauses_parents_final.json', 'r') as file:\n",
    "    clauses_data = json.load(file)\n",
    "\n",
    "with open('/Users/sunnyyu/Desktop/research/children_speech_books/code/python/generics/parents_generics/dependency_parsing/dependency_parsing_parents_final.json', 'r') as file:\n",
    "    parsing_data = json.load(file)\n",
    "\n",
    "# Create a mapping from sentences to categories\n",
    "sentence_to_category = {item['sentence']: item['category'] for item in clauses_data}\n",
    "\n",
    "# Add category to each entry in the dependency parsing data\n",
    "for entry in parsing_data:\n",
    "    sentence = entry['sentence']\n",
    "    # Assign category if the sentence is found in the mapping\n",
    "    if sentence in sentence_to_category:\n",
    "        entry['category'] = sentence_to_category[sentence]\n",
    "    else:\n",
    "        entry['category'] = None  # or some default value if no category is found\n",
    "\n",
    "# Save the updated data back to JSON\n",
    "with open('dependency_parsing_parents_final_updated.json', 'w') as file:\n",
    "    json.dump(parsing_data, file, indent=4)\n",
    "\n",
    "print(\"Categories added successfully.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
